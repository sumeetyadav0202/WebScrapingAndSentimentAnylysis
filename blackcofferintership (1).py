# -*- coding: utf-8 -*-
"""BlackcofferIntership.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16o75hijic8h0w6liscCRqSGcEBSlHd22
"""

from google.colab import drive

drive.mount('/content/gdrive')

import pandas as pd

df = pd.read_excel(r'/content/gdrive/MyDrive/assingment_intern/Input.xlsx')

display(pd.DataFrame(df))

import os
import requests
from bs4 import BeautifulSoup

# Loop through the rows in the dataframe
for index, row in df.iterrows():
    url_id = row['URL_ID']
    url = row['URL']

    # Make a request to the webpage and retrieve the content
    response = requests.get(url)
    webpage_content = response.content

    # Parse the webpage content using BeautifulSoup
    soup = BeautifulSoup(webpage_content, 'html.parser')  ## we are parseing so that we can create soup object wich will used to navigate and extract data

    # Find the element with class "td-post-content tagdiv-type"
    article_element = soup.find(class_="td-post-content tagdiv-type")

    # Extracting article title and article text if the article element is found in the soup
    # To Extract the article title
    article_title = soup.find("h1", class_="entry-title")
    if article_title:
        article_title = article_title.text.strip()  # To remove the leading and trailing whitespace


         # To Extract article text from the soup
        article_text = ""
        if article_element:
            for tag in article_element.find_all(['p', 'div']): # looping by finding the tag of 'p' and 'div'
                article_text += tag.get_text() + " "
        else:
            article_text = "Article text not found"

        # Saveing the extracted content into  text file with url_id as the file name
        file_name = f"{url_id}.txt"
        with open(file_name, 'w') as file:
            file.write("Article Title: " + article_title + "\n")
            file.write("Article Text: " + article_text + "\n")

        print(f"Content extracted from URL '{url}' and saved to '{file_name}'")
    else:
        print(f"Article element not found in URL '{url}'")

import glob # glob module provides function of wildcard wich can be used to search file using file path
from textblob import TextBlob # for doing sentiment anylysis
!pip install syllables
import syllables # for counting the syllables (syllables -> a word or part of a word which contains one vowel sound)

import nltk # open source libarary for doing NLP task
nltk.download('punkt') # Punkt tokenizer is a pre trained un supervised ml model used to tokenizer text into individual sentance and word

# Defineing a function to perform textual analysis on a given text file
def perform_text_analysis(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        content = file.read()
        # Extract title from file name (assuming file name format is "URLID_Title.txt")
        file_name = os.path.basename(file_path)
        url_id = file_name.split('_')[0]
        title = os.path.splitext('_'.join(file_name.split('_')[1:]))[0]
        
        # Performing sentiment analysis on the article content
        blob = TextBlob(content)
        positive_score = blob.sentiment.polarity
        negative_score = 1 - positive_score

        # Performing subjectivity analysis on the article content
        subjectivity_score = blob.sentiment.subjectivity

        # Computeing average sentence length on the article content
        sentences = blob.sentences
        avg_sentence_length = sum(len(sentence.words) for sentence in sentences) / len(sentences)

        # Computeing percentage of complex words on the article content
        words = blob.words
        complex_word_count = sum(syllables.estimate(word) >= 3 for word in words)
        percentage_complex_words = (complex_word_count / len(words)) * 100

        # Computeing FOG index on the article content
        # FOG index used to estimate the number of year of education one should have to read and understand the sentance
        fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)

        # Compute average number of words per sentence on the article content
        avg_words_per_sentence = len(words) / len(sentences)

        # Compute syllables per word on the article content
        syllables_per_word = sum(syllables.estimate(word) for word in words) / len(words)

        # Count personal pronouns on the article content
        personal_pronouns = sum(word.lower() in ['i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours'] for word in words)

        # Compute average word length on the article content
        avg_word_length = sum(len(word) for word in words) / len(words)

        # Create a dictionary to store the computed variables
        variables = {
            'URL_ID': url_id,
            'URL': '',
            'POSITIVE SCORE': positive_score,
            'NEGATIVE SCORE': negative_score,
            'POLARITY SCORE': positive_score - negative_score,
            'SUBJECTIVITY SCORE': subjectivity_score,
            'AVG SENTENCE LENGTH': avg_sentence_length,
            'PERCENTAGE OF COMPLEX WORDS': percentage_complex_words,
            'FOG INDEX': fog_index,
            'AVG NUMBER OF WORDS PER SENTENCE': avg_words_per_sentence,
            'COMPLEX WORD COUNT': complex_word_count,
            'WORD COUNT': len(words),
            'SYLLABLE PER WORD': syllables_per_word,
            'PERSONAL PRONOUNS': personal_pronouns,
            'AVG WORD LENGTH': avg_word_length
        }

        return variables

# Geting a list of file paths that match the pattern "*.txt"
file_paths = glob.glob('*.txt')

# Looping through the file paths and perform textual analysis for each file
output_data = []
for file_path in file_paths:
    variables = perform_text_analysis(file_path)
    output_data.append(variables)

# Create a DataFrame from the output data and save it to Excel
output_df = pd.DataFrame(output_data)
# Save the output DataFrame to an Excel file
output_df.to_excel('Output_Data_Structure.xlsx', index=False)

